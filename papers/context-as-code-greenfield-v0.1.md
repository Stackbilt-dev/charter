---
title: "Context-as-Code II: Measuring ADF Governance From Line Zero in a Greenfield Build"
paper-id: CSA-002
version: "0.1"
status: draft
date: 2026-02-26
authors:
  - Charter Kit Engineering
charter-version: "0.3.x"
baseline-source: "StackBilt Architect v2 parity tests (Anthropic, Gemini, Groq)"
subject-project: "Smart Revenue Rescue (SRR) Platform"
related:
  - paper-id: CSA-001
    relationship: "predecessor — retrofit measurement; this paper covers greenfield"
abstract: >
  CSA-001 proved ADF's effectiveness on a v1-to-v2 retrofit. This paper
  measures ADF governance applied from line zero on a greenfield build —
  Smart Revenue Rescue (SRR) — where the AI pipeline that planned the
  architecture also generated the governance constraints that governed
  its own build. Uniquely, baseline data was captured before development
  began: three independent LLM providers (Anthropic, Gemini, Groq) each
  produced a complete architecture plan from the same PRD, providing
  plan-vs-actual reconciliation data across every measurement axis.
---

# Context-as-Code II: Measuring ADF Governance From Line Zero in a Greenfield Build

A StackBilt Architect v2 + Charter Kit SDLC White-Paper
Date: February 2026
Status: DRAFT — Measurement rubric locked; data collection in progress.

## Premise

CSA-001 demonstrated that ADF reduces token payloads by 80% and enforces architectural invariants with 0% violation — but that study measured a retrofit where v1 failures were already cataloged. The natural question: **does ADF deliver the same results when applied from the start, before any technical debt exists to correct?**

This paper answers that question using a greenfield build of the Smart Revenue Rescue (SRR) platform. What makes this study unique is the existence of pre-development baseline data: before a single line of SRR code was written, three LLM providers independently generated complete architecture plans from the same PRD through the StackBilt Architect v2 pipeline. Those plans — including component counts, test scenario counts, ADR inventories, token costs, and deployable scaffolds — serve as the "predicted" baseline against which the actual ADF-governed build is measured.

## 1. Baseline Data (Pre-Development)

### 1.1 Source

Three architecture plans generated by StackBilt Architect v2 from the SRR PRD on 2026-02-23. Each plan traversed the full 6-mode pipeline: PRODUCT → UX → RISK → ARCHITECT → TDD → SPRINT.

### 1.2 Plan Inventory

| Metric | Anthropic | Gemini | Groq |
|---|---|---|---|
| Elapsed time | 2m46s | 2m50s | 2m19s |
| Total tokens (in+out) | 79,393 | 62,794 | 69,746 |
| Requirements produced | 22 | 23 | 16 |
| Risk items | 3 | 2 | 5 |
| Architecture components | 6 | 9 | 10 |
| Test scenarios | 10 | 25 | 16 |
| ADRs | 5 | 5 | 5 |
| Sprints | 3 | 2 | 2 |
| Quality pass | yes | yes | yes |

### 1.3 Scaffold Manifest

Each model's ARCHITECT mode produced a deployable scaffold. The file manifests serve as the predicted module inventory for plan-vs-actual comparison.

### 1.4 Governance Preflight

Enterprise governance preflight completed with:
- Quality score: 88/100 (post-refinement)
- Traceability: 100%
- Hard checks: 11/11 passed
- Domain lock: voiceops (7 required vendors)

This preflight data establishes the governance posture at the moment before development begins.

## 2. Measurement Rubric

All metrics are captured automatically by Charter CLI and the ADF evidence pipeline. No manual instrumentation required beyond standard CI integration (`charter adf evidence --auto-measure --append-log --ci`).

### 2.1 Context Economics

| Metric | How Measured | Frequency | CSA-001 Baseline |
|---|---|---|---|
| Baseline context tokens (DEFAULT_LOAD) | `adf bundle --format json` → `tokenEstimate` | Per task | ~300 tokens |
| Total context tokens (all loaded modules) | `adf bundle --format json` → `tokenEstimate` | Per task | ~1,484 tokens |
| Token budget utilization | `adf evidence` → `tokenUtilization` | Per CI run | 9% |
| Tokens-per-task trend | evidence-log.jsonl time series | Longitudinal | — (new metric) |
| Context growth rate vs codebase growth | token estimate / production LOC | Per milestone | — (new metric) |

**Key question:** Does context cost stay flat as the codebase grows (ADF routing working) or grow linearly (routing failing)?

### 2.2 Architectural Health

| Metric | How Measured | Frequency | CSA-001 Baseline |
|---|---|---|---|
| Module count | `ls src/**/*.ts` | Per milestone | 33 modules |
| Largest file LOC | `adf evidence --auto-measure` → max metric value | Per CI run | 343 LOC |
| Avg ceiling utilization % | mean(value/ceiling) across all metrics | Per CI run | ~70% est. |
| Ceiling headroom trend | ceiling utilization over time | Longitudinal | — (new metric) |
| God object formation | any file > 400 LOC | Per CI run | 0 violations |

**Key question:** Do modules stay within ceilings from the start, or does pressure build as features land?

### 2.3 Plan-vs-Actual Reconciliation (NEW — unique to CSA-002)

| Metric | Plan Source | Actual Source |
|---|---|---|
| Component count | Parity test: 6-10 components | Actual module count at each phase |
| File manifest | scaffold.json file list | Actual file inventory |
| Test count | Parity test: 10-25 scenarios | `vitest run` count at each phase |
| ADR count | Parity test: 5 ADRs per model | `charter validate` governed commits |
| Sprint structure | Parity test: 2-3 sprints | Actual phase boundaries |
| Token cost to plan vs execute | Parity test: 62K-79K | Cumulative `adf bundle` tokens across all tasks |

**Key question:** Which model's plan most accurately predicted the actual build? What's the typical expansion ratio from planned to actual?

### 2.4 ADF Routing Effectiveness

| Metric | How Measured | Frequency |
|---|---|---|
| Module trigger rate | `adf bundle --format json` → `triggerMatches` | Per task |
| Dead modules | `adf bundle` → `unmatchedModules` over time | Longitudinal |
| False negatives | Regressions in domain X when domain X module didn't trigger | Post-hoc analysis |
| Trigger keyword coverage | unique matched keywords / total defined triggers | Per milestone |

**Key question:** Are the manifest triggers accurate? Do on-demand modules fire when needed and stay quiet when not?

### 2.5 Governance Coverage

| Metric | How Measured | Frequency | CSA-001 Baseline |
|---|---|---|---|
| Test count | `vitest run` | Per CI run | 525 tests |
| Test pass rate | pass/total | Per CI run | 100% |
| Trailer coverage | `charter validate --format json` → governed % | Per CI run | — |
| Drift score | `charter drift --format json` → score | Per CI run | — |
| Evidence verdict | `charter adf evidence` → verdict | Per CI run | PASS |

### 2.6 Velocity Signal

| Metric | How Measured |
|---|---|
| Time from PRD to first deploy | Calendar days: PRD date → first `wrangler deploy` |
| Time per phase boundary | Calendar days between phase 1/2/3 milestones |
| Commits per phase | `git log --oneline` count between tags |
| LOC per phase | `adf evidence` metric deltas between milestones |

## 3. Data Collection Method

### 3.1 Evidence Ledger

Every CI run appends a JSON line to `.charter/evidence-log.jsonl`:

```bash
charter adf evidence --auto-measure --append-log --ci --format json
```

Each line contains: timestamp, constraint results, token estimate, budget utilization, weight summary, sync status, verdict, and auto-measured metric values.

### 3.2 Baseline Snapshot

At project initialization, `charter adf baseline` captures the day-zero state:

```json
{
  "capturedAt": "2026-02-...",
  "source": "architect-v2-scaffold",
  "plannedComponents": 9,
  "plannedFiles": ["wrangler.toml", "worker/index.ts", "routes/..."],
  "plannedTestScenarios": 25,
  "plannedADRs": 5,
  "plannedSprints": 2,
  "planTokenCost": 62794,
  "scaffoldHash": "..."
}
```

### 3.3 Milestone Snapshots

At each phase boundary (matching the PRD's 3-phase roadmap), a full snapshot is captured:

- `charter adf evidence --auto-measure --format json > .charter/snapshots/phase-N.json`
- `charter audit --format json >> .charter/snapshots/phase-N.json`
- Git tag applied: `v0.N.0`

### 3.4 Trend Report

At project completion, `charter adf trend` reads the evidence log + baseline and produces the plan-vs-actual reconciliation report that forms the core of this paper's findings.

## 4. Expected Findings (Hypotheses)

**H1 — Token flatness:** ADF context tokens per task will remain flat (<500) even as production LOC grows past 2,000. This would confirm that manifest routing scales.

**H2 — Ceiling compliance from day one:** 0% LOC ceiling violations throughout the build, matching CSA-001's result but without the corrective pressure of v1 god objects.

**H3 — Plan expansion ratio:** The actual build will produce 2-3x the components predicted by the scaffold, but the expansion will follow the existing component boundaries (new files within predicted domains, not new domains).

**H4 — Trigger accuracy:** >90% of on-demand module loads will be true positives (the task actually needed that context). <5% of tasks will show false-negative trigger misses.

**H5 — Model plan accuracy:** The model with the highest component count (Groq: 10) will most closely predict the actual module count, because finer decomposition better matches ADF's modular governance style.

## 5. Findings

*Data collection in progress. This section will be populated as the SRR build progresses through its three phases.*

### Phase 1: The Memory (Scorecard Engine)
_Pending_

### Phase 2: The Action (Revenue Doctor)
_Pending_

### Phase 3: The Optimization
_Pending_

### Plan-vs-Actual Reconciliation
_Pending_

## 6. Conclusion

_To be written after data collection is complete._

## Appendix A: Raw Baseline Data

Parity test summaries, scaffold manifests, and governance preflight snapshots are stored in the SRR repository at their original paths and referenced by SHA hash for reproducibility.

## Appendix B: Evidence Log Schema

```typescript
interface EvidenceLogEntry {
  timestamp: string;          // ISO 8601
  charterVersion: string;     // e.g. "0.3.1"
  modulesLoaded: string[];    // e.g. ["core.adf", "state.adf"]
  tokenEstimate: number;
  tokenBudget: number | null;
  tokenUtilization: number | null;
  constraints: Array<{
    key: string;
    value: number;
    ceiling: number;
    unit: string;
    status: 'pass' | 'warn' | 'fail';
  }>;
  weightSummary: {
    loadBearing: number;
    advisory: number;
    unweighted: number;
  };
  syncStatus: 'in_sync' | 'drifted' | 'no_lock';
  verdict: 'PASS' | 'WARN' | 'FAIL';
}
```
